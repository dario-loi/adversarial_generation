{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "import random\n",
    "\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from typing import List, Optional\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/dario/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemas & Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic classes for augmenting datasets\n",
    "\n",
    "class DataAugmentationStep(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __init__(self, probability: float):\n",
    "        self.probability = probability\n",
    "    \n",
    "    @abstractmethod\n",
    "    def apply(self, samples: List[Dict]) -> Optional[List[Dict]]:\n",
    "        pass\n",
    "\n",
    "class DataAugmentationPipeline:\n",
    "    \n",
    "    def __init__(self, steps: List[DataAugmentationStep]):\n",
    "        \"\"\"__init__ DataAugmentationPipeline \n",
    "        \n",
    "        A collection of data augmentation steps\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        steps : List[DataAugmentationStep]\n",
    "            A list of data augmentation steps, objects that implement the apply method\n",
    "        \"\"\"\n",
    "        self.steps = steps\n",
    "        \n",
    "        \n",
    "    def apply(self, sample: Dict) -> Optional[List[Dict]]:\n",
    "        \"\"\"apply Apply the pipeline to a sample\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample : Dict\n",
    "            A sample of our NLP dataset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict\n",
    "            An augmented sample\n",
    "        \"\"\"\n",
    "        \n",
    "        sample = [sample]\n",
    "        for step in self.steps:\n",
    "            sample = step.apply(sample)\n",
    "            if sample is None:\n",
    "                return None\n",
    "        return sample\n",
    "\n",
    "class DatasetAugmentation:\n",
    "    \n",
    "    def __init__(self, pipeline: DataAugmentationPipeline, percentage: float, random_sample: bool = False):\n",
    "        \"\"\"__init__ Constructor for the DatasetAugmentation class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pipeline : DataAugmentationPipeline\n",
    "            A pipeline of data augmentation steps\n",
    "        percentage : float\n",
    "            The percentage of the dataset to augment\n",
    "        random_sample : bool, optional\n",
    "            Whether to sample randomly from the dataset, if false the dataset gets converted to an \n",
    "            augmented version by extracting the indices sequentially from zero up to a given percentage\n",
    "            of its length, otherwise a given percentage of its indices get sampled without replacement, \n",
    "            by default False\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.pipeline = pipeline\n",
    "        self.percentage = percentage\n",
    "        self.random_sample = random_sample\n",
    "\n",
    "    def augment(self, dataset) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"augment Augment a dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : Dataset\n",
    "            A dataset object from the HuggingFace datasets library\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dataset\n",
    "            An augmented dataset\n",
    "        \"\"\"\n",
    "        n_samples = len(dataset)\n",
    "        n_samples_to_augment = int(n_samples * self.percentage)\n",
    "        if self.random_sample:\n",
    "            indices = np.random.choice(n_samples, n_samples_to_augment, replace=False)\n",
    "        else:\n",
    "            indices = np.arange(n_samples_to_augment)\n",
    "        augmented_samples = []\n",
    "        discarded = 0\n",
    "        \n",
    "        try:\n",
    "            for i in tqdm(indices):\n",
    "                sample = dataset[int(i)]\n",
    "                out = self.pipeline.apply(sample)\n",
    "                if out is not None and len(out) > 0:\n",
    "                    augmented_samples.extend([{\n",
    "                        'premise': augmented_sample['premise'],\n",
    "                        'hypothesis': augmented_sample['hypothesis'],\n",
    "                        'label': augmented_sample['label']\n",
    "                    } for augmented_sample in out]\n",
    "                )\n",
    "                else:\n",
    "                    discarded += 1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"Discarded {discarded} samples\")\n",
    "            raise e\n",
    "        \n",
    "        # convert list of dictionaries to pandas dataframe\n",
    "        augmented_dataset = pd.DataFrame.from_dict(augmented_samples)\n",
    "        \n",
    "        print(f\"Augmentation done, discarded {discarded} samples\")\n",
    "        return augmented_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Augmentation steps\n",
    "\n",
    "class Synonimization(DataAugmentationStep):\n",
    "    \n",
    "    def __init__(self, probability: float, apply_to: str, max_synonyms: int = 5):\n",
    "        \"\"\"__init__ Constructor for the Synonimization class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        probability : float\n",
    "            The probability of applying the step on each token\n",
    "        apply_to : str\n",
    "            The key of the sample to apply the step to, can be 'hypothesis' or 'premise'\n",
    "        max_synonyms : int, optional\n",
    "            The maximum number of synonyms to generate, by default 5\n",
    "        \"\"\"\n",
    "        self.probability = probability\n",
    "        self.apply_to = apply_to\n",
    "        self.max_synonyms = max_synonyms\n",
    "\n",
    "    def apply(self, samples: List[Dict]) -> Optional[List[Dict]]:\n",
    "        \"\"\"apply Apply the step to a set of samples\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        samples : List[Dict]\n",
    "            A set of samples from our NLP dataset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Optional[List[Dict]]\n",
    "            An augmented sample\n",
    "        \"\"\"\n",
    "        \n",
    "        result_set = []\n",
    "        \n",
    "        for sample in samples:\n",
    "            try:\n",
    "                \n",
    "                result_set.append(sample)\n",
    "                \n",
    "                for i, token in enumerate(sample[\"wsd\"][self.apply_to]):\n",
    "                    if np.random.rand() < self.probability:\n",
    "                        text = token[\"text\"]\n",
    "                        wsd_wnet = token[\"wnSynsetOffset\"]\n",
    "                        if wsd_wnet == \"O\":\n",
    "                            continue\n",
    "                        \n",
    "                        synonyms = self.get_synonym(text, wsd_wnet)\n",
    "\n",
    "                        for j, synonym in enumerate(synonyms):\n",
    "                            \n",
    "                            if synonym == text:\n",
    "                                continue\n",
    "                            \n",
    "                            if j >= self.max_synonyms:\n",
    "                                break\n",
    "                            \n",
    "                            new_sample = deepcopy(sample)\n",
    "                            new_sample[\"wsd\"][self.apply_to][i][\"text\"] = synonym\n",
    "                            new_sample[\"srl\"][self.apply_to][\"tokens\"][i][\"rawText\"] = synonym\n",
    "                            text = \" \".join([token[\"text\"] for token in new_sample[\"wsd\"][self.apply_to]])\n",
    "                            new_sample[self.apply_to] = text\n",
    "                            result_set.append(new_sample)\n",
    "            except:\n",
    "                print(f\"Error in sample: {sample}\")\n",
    "                print(f\"WSD: {sample[\"wsd\"][self.apply_to]}\")\n",
    "                print(f\"SRL: {sample[\"srl\"][self.apply_to]}\")\n",
    "                print(f\"Lengths: {len(sample[\"wsd\"][self.apply_to])}, {len(sample[\"srl\"][self.apply_to][\"tokens\"])}\")\n",
    "                raise ValueError(\"Error in sample\")\n",
    "                    \n",
    "        return result_set\n",
    "\n",
    "    def get_synonym(self, text, wsd_wnet):\n",
    "        \"\"\"get_synonym Get a synonym for a given word\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str\n",
    "            The word to find a synonym for\n",
    "        wsd_wnet : str\n",
    "            The WordNet synset offset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            A synonym for the given word\n",
    "        \"\"\"\n",
    "        \n",
    "        synsets = wn.synsets(text)\n",
    "        \n",
    "        strip_char = lambda s : int(\"\".join([c for c in s if c.isdigit()]))\n",
    "        \n",
    "        for synset in synsets:\n",
    "            if synset.offset() == strip_char(wsd_wnet):\n",
    "                synonyms = [\" \".join(w.name().split(\"_\")) for w in synset.lemmas()]\n",
    "                # shuffle\n",
    "                for synonym in random.sample(synonyms, len(synonyms)):\n",
    "                    yield synonym\n",
    "        yield text\n",
    "\n",
    "class CopulaInverter(DataAugmentationStep):\n",
    "    \n",
    "    def __init__(self, probability: float):\n",
    "        \"\"\"__init__ Constructor for the CopulaInverter class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        probability : float\n",
    "            The probability of applying the step on each token\n",
    "\n",
    "        \"\"\"\n",
    "        self.probability = probability\n",
    "        \n",
    "    def apply(self, samples: List[Dict]) -> Optional[List[Dict]]:\n",
    "        \"\"\"apply Apply the step to a sample\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample : List[Dict]\n",
    "            A sample of our NLP dataset (a list of dictionaries)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Optional[List[Dict]]\n",
    "            A set of augmented samples, or None if the sample is discarded\n",
    "        \"\"\"\n",
    "        result_set = []\n",
    "        \n",
    "        for sample in samples:\n",
    "            \n",
    "            if np.random.rand() > self.probability:\n",
    "                result_set.append(sample)\n",
    "                continue\n",
    "            \n",
    "            new_sample = deepcopy(sample)\n",
    "            \n",
    "            srl = new_sample[\"srl\"]\n",
    "            \n",
    "            existential_copula = None\n",
    "            \n",
    "            # linear search for an IS copula\n",
    "            for ann in srl[\"hypothesis\"][\"annotations\"]:\n",
    "                frame = ann[\"verbatlas\"][\"frameName\"]\n",
    "                token_lemma = srl[\"hypothesis\"][\"tokens\"][ann[\"tokenIndex\"]][\"rawText\"]\n",
    "            \n",
    "                if frame == \"COPULA\" and token_lemma == \"is\":\n",
    "                    existential_copula = ann\n",
    "                    break\n",
    "                    \n",
    "            if existential_copula is None:\n",
    "                result_set.append(new_sample)\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                slice_1 = existential_copula[\"verbatlas\"][\"roles\"][0][\"span\"]\n",
    "                slice_2 = existential_copula[\"verbatlas\"][\"roles\"][1][\"span\"]\n",
    "                \n",
    "                if slice_1[0] < slice_2[0]:\n",
    "                    slice_1, slice_2 = slice_2, slice_1\n",
    "                \n",
    "                slice_1 = slice(slice_1[0], slice_1[1])\n",
    "                slice_2 = slice(slice_2[0], slice_2[1])\n",
    "                \n",
    "            except IndexError:\n",
    "                print(\"Slice creation failed (role assumption failed?), skipping augmentation\")\n",
    "                print(f\"Sample was: {new_sample}\")\n",
    "                result_set.append(new_sample)\n",
    "                continue\n",
    "            \n",
    "            # swap the two slices in the wsd token list\n",
    "            \n",
    "            new_wsd = []\n",
    "            new_indices = list(range(len(sample[\"wsd\"][\"hypothesis\"])))\n",
    "            \n",
    "            # swap the two slices, hopefully we invert the sentence (passive -> active, active -> passive)\n",
    "            new_indices[slice_1], new_indices[slice_2] = new_indices[slice_2], new_indices[slice_1]\n",
    "\n",
    "            for i in new_indices:\n",
    "                new_wsd.append(sample[\"wsd\"][\"hypothesis\"][i])\n",
    "                \n",
    "            if len(new_wsd) != len(new_sample[\"wsd\"][\"hypothesis\"]):\n",
    "                # This should NEVER happen if we filtered the new_samples before in the pipeline\n",
    "                print(f\"Length mismatch: {len(new_wsd)} != {len(new_sample['wsd']['hypothesis'])}\")\n",
    "                print(f\"Sample was: {new_sample}\")\n",
    "                print(f\"Slice 1: {slice_1}, Slice 2: {slice_2}\")\n",
    "                text_new_sample = \" \".join([token[\"text\"] for token in new_sample[\"wsd\"][\"hypothesis\"]])\n",
    "                text_augmented = \" \".join([token[\"text\"] for token in new_wsd])\n",
    "                print(f\"Text new_sample: {text_new_sample}\")\n",
    "                print(f\"Text augmented: {text_augmented}\")\n",
    "            \n",
    "            new_sample[\"wsd\"][\"hypothesis\"] = new_wsd\n",
    "            new_sample[\"hypothesis\"] = \" \".join([token[\"text\"] for token in new_wsd])\n",
    "            \n",
    "            result_set.append(new_sample)\n",
    "\n",
    "        return result_set\n",
    "    \n",
    "class LengthFilter(DataAugmentationStep):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def apply(self, samples: List[Dict]) -> Optional[List[Dict]]:\n",
    "        \"\"\"apply Apply the step to a sample\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample : Dict\n",
    "            A sample of our NLP dataset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict\n",
    "            An augmented sample\n",
    "        \"\"\"\n",
    "        \n",
    "        predicate = lambda sample : (\n",
    "            len(sample[\"wsd\"][\"premise\"]) == len(sample[\"srl\"][\"premise\"][\"tokens\"]) \n",
    "            and \n",
    "            len(sample[\"wsd\"][\"hypothesis\"]) == len(sample[\"srl\"][\"hypothesis\"][\"tokens\"])\n",
    "            )\n",
    "        \n",
    "        return list(filter(predicate, samples))\n",
    "\n",
    "class CopulaContradictor(DataAugmentationStep):\n",
    "    \n",
    "    def __init__(self, probability: float):\n",
    "        \"\"\"__init__ Constructor for the CopulaContradictor class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        probability : float\n",
    "            The probability of applying the step on each token\n",
    "\n",
    "        \"\"\"\n",
    "        self.probability = probability\n",
    "        \n",
    "    def apply(self, samples: List[Dict]) -> Optional[List[Dict]]:\n",
    "        \"\"\"apply Apply the step to a sample\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample : List[Dict]\n",
    "            A sample of our NLP dataset (a list of dictionaries)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Optional[List[Dict]]\n",
    "            A set of augmented samples, or None if the sample is discarded\n",
    "        \"\"\"\n",
    "        result_set = []\n",
    "        \n",
    "        for sample in samples:\n",
    "            \n",
    "            if np.random.rand() > self.probability:\n",
    "                result_set.append(sample)\n",
    "                continue\n",
    "            \n",
    "            new_sample = deepcopy(sample)\n",
    "            \n",
    "            srl = new_sample[\"srl\"]\n",
    "            \n",
    "            existential_copula = None\n",
    "            \n",
    "            # linear search for an IS copula\n",
    "            for ann in srl[\"hypothesis\"][\"annotations\"]:\n",
    "                frame = ann[\"verbatlas\"][\"frameName\"]\n",
    "                token_lemma = srl[\"hypothesis\"][\"tokens\"][ann[\"tokenIndex\"]][\"rawText\"]\n",
    "            \n",
    "                if frame == \"COPULA\" and token_lemma == \"is\":\n",
    "                    existential_copula = ann\n",
    "                    break\n",
    "                    \n",
    "            if existential_copula is None:\n",
    "                result_set.append(new_sample)\n",
    "                continue\n",
    "        \n",
    "            srl[\"hypothesis\"][\"tokens\"][ann[\"tokenIndex\"]][\"rawText\"] = \"is not\"\n",
    "            new_sample[\"label\"] = (\n",
    "            \"ENTAILMENT\" if new_sample[\"label\"] == \"CONTRADICTION\" \n",
    "                            else \"CONTRADICTION\" if new_sample[\"label\"] == \"ENTAILMENT\" \n",
    "                            else new_sample[\"label\"]\n",
    "            )\n",
    "            text = \" \".join([token[\"rawText\"] for token in srl[\"hypothesis\"][\"tokens\"]])\n",
    "            new_sample[\"hypothesis\"] = text\n",
    "            new_sample[\"wsd\"][\"hypothesis\"][ann[\"tokenIndex\"]][\"text\"] = \"is not\"\n",
    "\n",
    "            result_set.append(new_sample)\n",
    "            \n",
    "        return result_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(sample):\n",
    "    \n",
    "    premise = sample[\"premise\"]\n",
    "    hypothesis = sample[\"hypothesis\"]\n",
    "    wsd_premise = sample[\"wsd\"][\"premise\"]\n",
    "    wsd_hypothesis = sample[\"wsd\"][\"hypothesis\"]\n",
    "    \n",
    "    print(\"Premise\")\n",
    "    print(premise)\n",
    "    print(\"Hypothesis\")\n",
    "    print(hypothesis)\n",
    "    print(\"WSD Premise\")\n",
    "    print(\" \".join([token[\"text\"] for token in wsd_premise]))\n",
    "    print(\"WSD Hypothesis\")\n",
    "    print(\" \".join([token[\"text\"] for token in wsd_hypothesis]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "augmentor = DatasetAugmentation(\n",
    "    pipeline=DataAugmentationPipeline([\n",
    "        LengthFilter(),\n",
    "        Synonimization(probability=0.15, apply_to=\"premise\", max_synonyms=2),\n",
    "        CopulaInverter(probability=0.5),\n",
    "        CopulaContradictor(probability=0.2),\n",
    "        Synonimization(probability=0.2, apply_to=\"hypothesis\", max_synonyms=3),\n",
    "    ]),\n",
    "    percentage=0.05,\n",
    "    random_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"roberta-base\"\n",
    "DATASET_NAME = \"tommasobonomo/sem_augmented_fever_nli\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ds[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(ds, idx):\n",
    "    for f in list((\"premise\", \"hypothesis\", \"label\")):\n",
    "        print(f\"{f}: {ds[f][idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2554/2554 [00:17<00:00, 149.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation done, discarded 1064 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_train_data = augmentor.augment(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Roman Atwood . He is best known for his vlogs ...</td>\n",
       "      <td>a content creator is Roman Atwood .</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Roman Atwood . He is best know for his vlogs ,...</td>\n",
       "      <td>Roman Atwood is not a content creator .</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Roman Atwood . He is best know for his vlogs ,...</td>\n",
       "      <td>Roman Atwood is not a contented creator .</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Roman Atwood . He is best know for his vlogs ,...</td>\n",
       "      <td>Roman Atwood is not a content creator .</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Roman Atwood . He is best know for his vlogs ,...</td>\n",
       "      <td>Roman Atwood is not a content creator .</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Roman Atwood . He is best known for his vlogs ...</td>\n",
       "      <td>Roman Atwood is not a content creator .</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Roman Atwood . He is best known for his vlogs ...</td>\n",
       "      <td>a content creator is Roman Atwood .</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Roman Atwood . He is best known for his vlogs ...</td>\n",
       "      <td>Roman Atwood is a content creator.</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Roman Atwood . He is best known for his vlogs ...</td>\n",
       "      <td>Roman Atwood is a contented creator .</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Roman Atwood . He is best known for his vlogs ...</td>\n",
       "      <td>Roman Atwood is a content creator .</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Roman Atwood . He is best known for his vlogs ...</td>\n",
       "      <td>Roman Atwood is a content creator .</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Roman Atwood . He is best known for his vlogs ...</td>\n",
       "      <td>a content creator is Roman Atwood .</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Roman Atwood . He is best known for his vlogs ...</td>\n",
       "      <td>Roman Atwood is a content creator.</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Roman Atwood . He is best known for his vlogs ...</td>\n",
       "      <td>Roman Atwood is not a content creator .</td>\n",
       "      <td>CONTRADICTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Roman Atwood . He is best known for his vlogs ...</td>\n",
       "      <td>Roman Atwood is a content creator.</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The Hunger Games is a 2012 American dystopian ...</td>\n",
       "      <td>There is a movie called The Hunger Games.</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The Hunger Games is a 2012 American dystopian ...</td>\n",
       "      <td>There is a pic called The Hunger Games .</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>The Hunger Games is a 2012 American dystopian ...</td>\n",
       "      <td>There is a motion-picture show called The Hung...</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The Hunger Games is a 2012 American dystopian ...</td>\n",
       "      <td>There is a moving picture called The Hunger Ga...</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The Hunger Games is a 2012 American dystopian ...</td>\n",
       "      <td>There is a movie call The Hunger Games .</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              premise  \\\n",
       "0   Roman Atwood . He is best known for his vlogs ...   \n",
       "1   Roman Atwood . He is best know for his vlogs ,...   \n",
       "2   Roman Atwood . He is best know for his vlogs ,...   \n",
       "3   Roman Atwood . He is best know for his vlogs ,...   \n",
       "4   Roman Atwood . He is best know for his vlogs ,...   \n",
       "5   Roman Atwood . He is best known for his vlogs ...   \n",
       "6   Roman Atwood . He is best known for his vlogs ...   \n",
       "7   Roman Atwood . He is best known for his vlogs ...   \n",
       "8   Roman Atwood . He is best known for his vlogs ...   \n",
       "9   Roman Atwood . He is best known for his vlogs ...   \n",
       "10  Roman Atwood . He is best known for his vlogs ...   \n",
       "11  Roman Atwood . He is best known for his vlogs ...   \n",
       "12  Roman Atwood . He is best known for his vlogs ...   \n",
       "13  Roman Atwood . He is best known for his vlogs ...   \n",
       "14  Roman Atwood . He is best known for his vlogs ...   \n",
       "15  The Hunger Games is a 2012 American dystopian ...   \n",
       "16  The Hunger Games is a 2012 American dystopian ...   \n",
       "17  The Hunger Games is a 2012 American dystopian ...   \n",
       "18  The Hunger Games is a 2012 American dystopian ...   \n",
       "19  The Hunger Games is a 2012 American dystopian ...   \n",
       "\n",
       "                                           hypothesis          label  \n",
       "0                 a content creator is Roman Atwood .     ENTAILMENT  \n",
       "1             Roman Atwood is not a content creator .  CONTRADICTION  \n",
       "2           Roman Atwood is not a contented creator .  CONTRADICTION  \n",
       "3             Roman Atwood is not a content creator .  CONTRADICTION  \n",
       "4             Roman Atwood is not a content creator .  CONTRADICTION  \n",
       "5             Roman Atwood is not a content creator .  CONTRADICTION  \n",
       "6                 a content creator is Roman Atwood .     ENTAILMENT  \n",
       "7                  Roman Atwood is a content creator.     ENTAILMENT  \n",
       "8               Roman Atwood is a contented creator .     ENTAILMENT  \n",
       "9                 Roman Atwood is a content creator .     ENTAILMENT  \n",
       "10                Roman Atwood is a content creator .     ENTAILMENT  \n",
       "11                a content creator is Roman Atwood .     ENTAILMENT  \n",
       "12                 Roman Atwood is a content creator.     ENTAILMENT  \n",
       "13            Roman Atwood is not a content creator .  CONTRADICTION  \n",
       "14                 Roman Atwood is a content creator.     ENTAILMENT  \n",
       "15          There is a movie called The Hunger Games.     ENTAILMENT  \n",
       "16           There is a pic called The Hunger Games .     ENTAILMENT  \n",
       "17  There is a motion-picture show called The Hung...     ENTAILMENT  \n",
       "18  There is a moving picture called The Hunger Ga...     ENTAILMENT  \n",
       "19           There is a movie call The Hunger Games .     ENTAILMENT  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_train_data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2965"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_train_data[\"id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.740303541315345"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(augmented_train_data)/augmented_train_data[\"id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- AUGMENTED ---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'wsd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/repos/MNLP_HW3/.mnlp/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'wsd'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m IDX \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- AUGMENTED ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([token[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43maugmented_train_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mIDX\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwsd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpremise\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([token[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m augmented_train_data\u001b[38;5;241m.\u001b[39miloc[IDX][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwsd\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhypothesis\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m~/repos/MNLP_HW3/.mnlp/lib/python3.12/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/repos/MNLP_HW3/.mnlp/lib/python3.12/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/repos/MNLP_HW3/.mnlp/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'wsd'"
     ]
    }
   ],
   "source": [
    "IDX = 1000\n",
    "\n",
    "print(\"--- AUGMENTED ---\")\n",
    "print(\n",
    "\" \".join([token[\"text\"] for token in augmented_train_data.iloc[IDX][\"wsd\"][\"premise\"]])\n",
    ")\n",
    "print(\n",
    "\" \".join([token[\"text\"] for token in augmented_train_data.iloc[IDX][\"wsd\"][\"hypothesis\"]])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_data.iloc[IDX][\"wsd\"][\"premise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(augmented_train_data.iloc[314])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synset = wn.synsets(\"number\")\n",
    "print(synset)\n",
    "synset = wn.synsets(\"number\")[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'synset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msynset\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'synset' is not defined"
     ]
    }
   ],
   "source": [
    "synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'synset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m [w\u001b[38;5;241m.\u001b[39mname() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[43msynset\u001b[49m\u001b[38;5;241m.\u001b[39mlemmas()]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'synset' is not defined"
     ]
    }
   ],
   "source": [
    "[w.name() for w in synset.lemmas()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'synset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(w\u001b[38;5;241m.\u001b[39mname()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[43msynset\u001b[49m\u001b[38;5;241m.\u001b[39mlemmas()]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'synset' is not defined"
     ]
    }
   ],
   "source": [
    "[\" \".join(w.name().split(\"_\")) for w in synset.lemmas()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premise: David Lemieux ( born December 22 , 1988 ) is a Canadian professional boxer who held the IBF middleweight title in 2015 . Gennady Golovkin . Later that year he defeated Marco Antonio Rubio to add the WBC interim middleweight title to his collection , and defeated David Lemieux in 2015 to win the IBF middleweight title . The World Boxing Council ( WBC ) is one of four major organizations which sanction world championship boxing bouts , alongside the International Boxing Federation ( IBF ) , World Boxing Association ( WBA ) and World Boxing Organization ( WBO ) .\n",
      "hypothesis: Gennady Golovkin boxes.\n",
      "label: ENTAILMENT\n"
     ]
    }
   ],
   "source": [
    "print_sample(train_data, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 0,\n",
       " 'text': 'Roman',\n",
       " 'pos': 'PROPN',\n",
       " 'lemma': 'Roman',\n",
       " 'bnSynsetId': 'O',\n",
       " 'wnSynsetOffset': 'O',\n",
       " 'nltkSynset': 'O'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][\"wsd\"][\"hypothesis\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [{'index': 0, 'rawText': 'Roman'},\n",
       "  {'index': 1, 'rawText': 'Atwood'},\n",
       "  {'index': 2, 'rawText': 'is'},\n",
       "  {'index': 3, 'rawText': 'a'},\n",
       "  {'index': 4, 'rawText': 'content'},\n",
       "  {'index': 5, 'rawText': 'creator'},\n",
       "  {'index': 6, 'rawText': '.'}],\n",
       " 'annotations': [{'tokenIndex': 2,\n",
       "   'verbatlas': {'frameName': 'COPULA',\n",
       "    'roles': [{'role': 'Theme', 'score': 1.0, 'span': [0, 2]},\n",
       "     {'role': 'Attribute', 'score': 1.0, 'span': [3, 6]}]},\n",
       "   'englishPropbank': {'frameName': 'be.01',\n",
       "    'roles': [{'role': 'ARG1', 'score': 1.0, 'span': [0, 2]},\n",
       "     {'role': 'ARG2', 'score': 1.0, 'span': [3, 6]}]}}]}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][\"srl\"][\"hypothesis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frameName': 'COPULA',\n",
       " 'roles': [{'role': 'Theme', 'score': 1.0, 'span': [0, 2]},\n",
       "  {'role': 'Attribute', 'score': 1.0, 'span': [3, 6]}]}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][\"srl\"][\"hypothesis\"][\"annotations\"][0][\"verbatlas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice1 = train_data[0][\"srl\"][\"hypothesis\"][\"annotations\"][0][\"verbatlas\"][\"roles\"][0][\"span\"]\n",
    "slice2 = train_data[0][\"srl\"][\"hypothesis\"][\"annotations\"][0][\"verbatlas\"][\"roles\"][1][\"span\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0][\"srl\"][\"hypothesis\"][\"tokens\"][slice(slice1[0], slice1[1])]\n",
    "train_data[0][\"srl\"][\"hypothesis\"][\"tokens\"][slice(slice2[0], slice2[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def negate_hypothesis(sample):\n",
    "    srl = sample[\"srl\"]\n",
    "    \n",
    "    # detect IS verb\n",
    "    \n",
    "    for ann in srl[\"hypothesis\"][\"annotations\"]:\n",
    "        frame = ann[\"verbatlas\"][\"frameName\"]\n",
    "        token_lemma = srl[\"hypothesis\"][\"tokens\"][ann[\"tokenIndex\"]][\"rawText\"]\n",
    "        \n",
    "\n",
    "        if frame == \"COPULA\" and token_lemma == \"is\":\n",
    "            print(\"Can be negated\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negate_hypothesis(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0][\"srl\"][\"hypothesis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0][\"wsd\"][\"hypothesis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(train_data[0][\"wsd\"][\"hypothesis\"])))\n",
    "new_indices = indices.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_indices[3:6], new_indices[0:2]  = new_indices[0:2], new_indices[3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join([train_data[0][\"wsd\"][\"hypothesis\"][i][\"text\"] for i in new_indices ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
