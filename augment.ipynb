{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "import random\n",
    "\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from typing import List, Optional\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemas & Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic classes for augmenting datasets\n",
    "\n",
    "class DataAugmentationStep(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __init__(self, probability: float):\n",
    "        self.probability = probability\n",
    "    \n",
    "    @abstractmethod\n",
    "    def apply(self, samples: List[Dict]) -> Optional[List[Dict]]:\n",
    "        pass\n",
    "\n",
    "class DataAugmentationPipeline:\n",
    "    \n",
    "    def __init__(self, steps: List[DataAugmentationStep]):\n",
    "        \"\"\"__init__ DataAugmentationPipeline \n",
    "        \n",
    "        A collection of data augmentation steps\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        steps : List[DataAugmentationStep]\n",
    "            A list of data augmentation steps, objects that implement the apply method\n",
    "        \"\"\"\n",
    "        self.steps = steps\n",
    "        \n",
    "        \n",
    "    def apply(self, sample: Dict) -> Optional[List[Dict]]:\n",
    "        \"\"\"apply Apply the pipeline to a sample\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample : Dict\n",
    "            A sample of our NLP dataset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict\n",
    "            An augmented sample\n",
    "        \"\"\"\n",
    "        \n",
    "        sample = [sample]\n",
    "        for step in self.steps:\n",
    "            sample = step.apply(sample)\n",
    "            if sample is None:\n",
    "                return None\n",
    "        return sample\n",
    "\n",
    "class DatasetAugmentation:\n",
    "    \n",
    "    def __init__(self, pipeline: DataAugmentationPipeline, percentage: float, random_sample: bool = False):\n",
    "        \"\"\"__init__ Constructor for the DatasetAugmentation class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pipeline : DataAugmentationPipeline\n",
    "            A pipeline of data augmentation steps\n",
    "        percentage : float\n",
    "            The percentage of the dataset to augment\n",
    "        random_sample : bool, optional\n",
    "            Whether to sample randomly from the dataset, if false the dataset gets converted to an \n",
    "            augmented version by extracting the indices sequentially from zero up to a given percentage\n",
    "            of its length, otherwise a given percentage of its indices get sampled without replacement, \n",
    "            by default False\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.pipeline = pipeline\n",
    "        self.percentage = percentage\n",
    "        self.random_sample = random_sample\n",
    "\n",
    "    def augment(self, dataset) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"augment Augment a dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : Dataset\n",
    "            A dataset object from the HuggingFace datasets library\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dataset\n",
    "            An augmented dataset\n",
    "        \"\"\"\n",
    "        n_samples = len(dataset)\n",
    "        n_samples_to_augment = int(n_samples * self.percentage)\n",
    "        if self.random_sample:\n",
    "            indices = np.random.choice(n_samples, n_samples_to_augment, replace=False)\n",
    "        else:\n",
    "            indices = np.arange(n_samples_to_augment)\n",
    "        augmented_samples = []\n",
    "        discarded = 0\n",
    "        \n",
    "        try:\n",
    "            for i in tqdm(indices):\n",
    "                sample = dataset.iloc[i].to_dict()\n",
    "                out = self.pipeline.apply(sample)\n",
    "                if out is not None and len(out) > 0:\n",
    "                    augmented_samples.extend([{\n",
    "                        'premise': augmented_sample['premise'],\n",
    "                        'hypothesis': augmented_sample['hypothesis'],\n",
    "                        'label': augmented_sample['label']\n",
    "                    } for augmented_sample in out]\n",
    "                )\n",
    "                else:\n",
    "                    discarded += 1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"Discarded {discarded} samples\")\n",
    "            raise e\n",
    "        \n",
    "        # convert list of dictionaries to pandas dataframe\n",
    "        augmented_dataset = pd.DataFrame.from_dict(augmented_samples)\n",
    "        \n",
    "        print(f\"Augmentation done, discarded {discarded} samples\")\n",
    "        return augmented_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Augmentation steps\n",
    "\n",
    "class Synonimization(DataAugmentationStep):\n",
    "    \n",
    "    def __init__(self, probability: float, apply_to: str, max_synonyms: int = 5):\n",
    "        \"\"\"__init__ Constructor for the Synonimization class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        probability : float\n",
    "            The probability of applying the step on each token\n",
    "        apply_to : str\n",
    "            The key of the sample to apply the step to, can be 'hypothesis' or 'premise'\n",
    "        max_synonyms : int, optional\n",
    "            The maximum number of synonyms to generate, by default 5\n",
    "        \"\"\"\n",
    "        self.probability = probability\n",
    "        self.apply_to = apply_to\n",
    "        self.max_synonyms = max_synonyms\n",
    "\n",
    "    def apply(self, samples: List[Dict]) -> Optional[List[Dict]]:\n",
    "        \"\"\"apply Apply the step to a set of samples\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        samples : List[Dict]\n",
    "            A set of samples from our NLP dataset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Optional[List[Dict]]\n",
    "            An augmented sample\n",
    "        \"\"\"\n",
    "        \n",
    "        result_set = []\n",
    "        \n",
    "        for sample in samples:\n",
    "            try:\n",
    "                \n",
    "                result_set.append(sample)\n",
    "                \n",
    "                for i, token in enumerate(sample[\"wsd\"][self.apply_to]):\n",
    "                    if np.random.rand() < self.probability:\n",
    "                        text = token[\"text\"]\n",
    "                        wsd_wnet = token[\"wnSynsetOffset\"]\n",
    "                        if wsd_wnet == \"O\":\n",
    "                            continue\n",
    "                        \n",
    "                        synonyms = self.get_synonym(text, wsd_wnet)\n",
    "\n",
    "                        for j, synonym in enumerate(synonyms):\n",
    "                            \n",
    "                            if synonym == text:\n",
    "                                continue\n",
    "                            \n",
    "                            if j >= self.max_synonyms:\n",
    "                                break\n",
    "                            \n",
    "                            new_sample = deepcopy(sample)\n",
    "                            new_sample[\"wsd\"][self.apply_to][i][\"text\"] = synonym\n",
    "                            new_sample[\"srl\"][self.apply_to][\"tokens\"][i][\"rawText\"] = synonym\n",
    "                            text = \" \".join([token[\"text\"] for token in new_sample[\"wsd\"][self.apply_to]])\n",
    "                            new_sample[self.apply_to] = text\n",
    "                            result_set.append(new_sample)\n",
    "            except:\n",
    "                print(f\"Error in sample: {sample}\")\n",
    "                print(f\"WSD: {sample[\"wsd\"][self.apply_to]}\")\n",
    "                print(f\"SRL: {sample[\"srl\"][self.apply_to]}\")\n",
    "                print(f\"Lengths: {len(sample[\"wsd\"][self.apply_to])}, {len(sample[\"srl\"][self.apply_to][\"tokens\"])}\")\n",
    "                raise ValueError(\"Error in sample\")\n",
    "                    \n",
    "        return result_set\n",
    "\n",
    "    def get_synonym(self, text, wsd_wnet):\n",
    "        \"\"\"get_synonym Get a synonym for a given word\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str\n",
    "            The word to find a synonym for\n",
    "        wsd_wnet : str\n",
    "            The WordNet synset offset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            A synonym for the given word\n",
    "        \"\"\"\n",
    "        \n",
    "        synsets = wn.synsets(text)\n",
    "        \n",
    "        strip_char = lambda s : int(\"\".join([c for c in s if c.isdigit()]))\n",
    "        \n",
    "        for synset in synsets:\n",
    "            if synset.offset() == strip_char(wsd_wnet):\n",
    "                synonyms = [\" \".join(w.name().split(\"_\")) for w in synset.lemmas()]\n",
    "                # shuffle\n",
    "                for synonym in random.sample(synonyms, len(synonyms)):\n",
    "                    yield synonym\n",
    "        yield text\n",
    "\n",
    "class CopulaInverter(DataAugmentationStep):\n",
    "    \n",
    "    def __init__(self, probability: float):\n",
    "        \"\"\"__init__ Constructor for the CopulaInverter class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        probability : float\n",
    "            The probability of applying the step on each token\n",
    "\n",
    "        \"\"\"\n",
    "        self.probability = probability\n",
    "        \n",
    "    def apply(self, samples: List[Dict]) -> Optional[List[Dict]]:\n",
    "        \"\"\"apply Apply the step to a sample\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample : List[Dict]\n",
    "            A sample of our NLP dataset (a list of dictionaries)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Optional[List[Dict]]\n",
    "            A set of augmented samples, or None if the sample is discarded\n",
    "        \"\"\"\n",
    "        result_set = []\n",
    "        \n",
    "        for sample in samples:\n",
    "            \n",
    "            if np.random.rand() > self.probability:\n",
    "                result_set.append(sample)\n",
    "                continue\n",
    "            \n",
    "            new_sample = deepcopy(sample)\n",
    "            \n",
    "            srl = new_sample[\"srl\"]\n",
    "            \n",
    "            existential_copula = None\n",
    "            \n",
    "            # linear search for an IS copula\n",
    "            for ann in srl[\"hypothesis\"][\"annotations\"]:\n",
    "                frame = ann[\"verbatlas\"][\"frameName\"]\n",
    "                token_lemma = srl[\"hypothesis\"][\"tokens\"][ann[\"tokenIndex\"]][\"rawText\"]\n",
    "            \n",
    "                if frame == \"COPULA\" and (token_lemma == \"is\" or token_lemma == \"was\" or token_lemma == \"were\" or token_lemma == \"are\"):\n",
    "                    \n",
    "                    existential_copula = ann\n",
    "                    break\n",
    "                    \n",
    "            if existential_copula is None:\n",
    "                result_set.append(new_sample)\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                slice_1 = existential_copula[\"verbatlas\"][\"roles\"][0][\"span\"]\n",
    "                slice_2 = existential_copula[\"verbatlas\"][\"roles\"][1][\"span\"]\n",
    "                \n",
    "                if slice_1[0] < slice_2[0]:\n",
    "                    slice_1, slice_2 = slice_2, slice_1\n",
    "                \n",
    "                slice_1 = slice(slice_1[0], slice_1[1])\n",
    "                slice_2 = slice(slice_2[0], slice_2[1])\n",
    "                \n",
    "            except IndexError:\n",
    "                # Somehow the copula does not have the right spans, we skip the augmentation\n",
    "                # and keep the sample as is\n",
    "                result_set.append(new_sample)\n",
    "                continue\n",
    "            \n",
    "            # swap the two slices in the wsd token list\n",
    "            \n",
    "            new_wsd = []\n",
    "            new_indices = list(range(len(sample[\"wsd\"][\"hypothesis\"])))\n",
    "            \n",
    "            # swap the two slices, hopefully we invert the sentence (passive -> active, active -> passive)\n",
    "            new_indices[slice_1], new_indices[slice_2] = new_indices[slice_2], new_indices[slice_1]\n",
    "\n",
    "            for i in new_indices:\n",
    "                new_wsd.append(sample[\"wsd\"][\"hypothesis\"][i])\n",
    "                \n",
    "            if len(new_wsd) != len(new_sample[\"wsd\"][\"hypothesis\"]):\n",
    "                # This should NEVER happen if we filtered the new_samples before in the pipeline\n",
    "                print(f\"Length mismatch: {len(new_wsd)} != {len(new_sample['wsd']['hypothesis'])}\")\n",
    "                print(f\"Sample was: {new_sample}\")\n",
    "                print(f\"Slice 1: {slice_1}, Slice 2: {slice_2}\")\n",
    "                text_new_sample = \" \".join([token[\"text\"] for token in new_sample[\"wsd\"][\"hypothesis\"]])\n",
    "                text_augmented = \" \".join([token[\"text\"] for token in new_wsd])\n",
    "                print(f\"Text new_sample: {text_new_sample}\")\n",
    "                print(f\"Text augmented: {text_augmented}\")\n",
    "            \n",
    "            new_sample[\"wsd\"][\"hypothesis\"] = new_wsd\n",
    "            new_sample[\"hypothesis\"] = \" \".join([token[\"text\"] for token in new_wsd])\n",
    "            \n",
    "            result_set.append(new_sample)\n",
    "\n",
    "        return result_set\n",
    "    \n",
    "class LengthFilter(DataAugmentationStep):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def apply(self, samples: List[Dict]) -> Optional[List[Dict]]:\n",
    "        \"\"\"apply Apply the step to a sample\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample : Dict\n",
    "            A sample of our NLP dataset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict\n",
    "            An augmented sample\n",
    "        \"\"\"\n",
    "        \n",
    "        predicate = lambda sample : (\n",
    "            len(sample[\"wsd\"][\"premise\"]) == len(sample[\"srl\"][\"premise\"][\"tokens\"]) \n",
    "            and \n",
    "            len(sample[\"wsd\"][\"hypothesis\"]) == len(sample[\"srl\"][\"hypothesis\"][\"tokens\"])\n",
    "            )\n",
    "        \n",
    "        return list(filter(predicate, samples))\n",
    "\n",
    "class CopulaContradictor(DataAugmentationStep):\n",
    "    \n",
    "    def __init__(self, probability: float):\n",
    "        \"\"\"__init__ Constructor for the CopulaContradictor class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        probability : float\n",
    "            The probability of applying the step on each token\n",
    "\n",
    "        \"\"\"\n",
    "        self.probability = probability\n",
    "        \n",
    "    def apply(self, samples: List[Dict]) -> Optional[List[Dict]]:\n",
    "        \"\"\"apply Apply the step to a sample\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample : List[Dict]\n",
    "            A sample of our NLP dataset (a list of dictionaries)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Optional[List[Dict]]\n",
    "            A set of augmented samples, or None if the sample is discarded\n",
    "        \"\"\"\n",
    "        result_set = []\n",
    "        \n",
    "        for sample in samples:\n",
    "            \n",
    "            if np.random.rand() > self.probability:\n",
    "                result_set.append(sample)\n",
    "                continue\n",
    "            \n",
    "            new_sample = deepcopy(sample)\n",
    "            \n",
    "            srl = new_sample[\"srl\"]\n",
    "            \n",
    "            existential_copula = None\n",
    "            \n",
    "            # linear search for an IS copula\n",
    "            for ann in srl[\"hypothesis\"][\"annotations\"]:\n",
    "                frame = ann[\"verbatlas\"][\"frameName\"]\n",
    "                token_lemma = srl[\"hypothesis\"][\"tokens\"][ann[\"tokenIndex\"]][\"rawText\"]\n",
    "            \n",
    "                if frame == \"COPULA\" and token_lemma == \"is\":\n",
    "                    existential_copula = ann\n",
    "                    break\n",
    "                    \n",
    "            if existential_copula is None:\n",
    "                result_set.append(new_sample)\n",
    "                continue\n",
    "        \n",
    "            srl[\"hypothesis\"][\"tokens\"][ann[\"tokenIndex\"]][\"rawText\"] = \"is not\"\n",
    "            new_sample[\"label\"] = (\n",
    "            \"ENTAILMENT\" if new_sample[\"label\"] == \"CONTRADICTION\" \n",
    "                            else \"CONTRADICTION\" if new_sample[\"label\"] == \"ENTAILMENT\" \n",
    "                            else new_sample[\"label\"]\n",
    "            )\n",
    "            text = \" \".join([token[\"rawText\"] for token in srl[\"hypothesis\"][\"tokens\"]])\n",
    "            new_sample[\"hypothesis\"] = text\n",
    "            new_sample[\"wsd\"][\"hypothesis\"][ann[\"tokenIndex\"]][\"text\"] = \"is not\"\n",
    "\n",
    "            result_set.append(new_sample)\n",
    "            \n",
    "        return result_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(sample):\n",
    "    \n",
    "    premise = sample[\"premise\"]\n",
    "    hypothesis = sample[\"hypothesis\"]\n",
    "    wsd_premise = sample[\"wsd\"][\"premise\"]\n",
    "    wsd_hypothesis = sample[\"wsd\"][\"hypothesis\"]\n",
    "    \n",
    "    print(\"Premise\")\n",
    "    print(premise)\n",
    "    print(\"Hypothesis\")\n",
    "    print(hypothesis)\n",
    "    print(\"WSD Premise\")\n",
    "    print(\" \".join([token[\"text\"] for token in wsd_premise]))\n",
    "    print(\"WSD Hypothesis\")\n",
    "    print(\" \".join([token[\"text\"] for token in wsd_hypothesis]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SUBSET_PERCENT = 1.00\n",
    "augmentor = DatasetAugmentation(\n",
    "    pipeline=DataAugmentationPipeline([\n",
    "        LengthFilter(),\n",
    "        Synonimization(probability=0.3, apply_to=\"premise\", max_synonyms=2),\n",
    "        CopulaInverter(probability=0.5),\n",
    "        CopulaContradictor(probability=0.25),\n",
    "        Synonimization(probability=0.35, apply_to=\"hypothesis\", max_synonyms=3),\n",
    "    ]),\n",
    "    percentage=SUBSET_PERCENT,\n",
    "    random_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(ds, idx):\n",
    "    for f in list((\"premise\", \"hypothesis\", \"label\")):\n",
    "        print(f\"{f}: {ds[f][idx]}\")\n",
    "\n",
    "DATASET_NAME = \"tommasobonomo/sem_augmented_fever_nli\"\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ds[\"train\"].to_pandas()\n",
    "val_data = ds[\"validation\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate augmentation for separate files, we do not want to leak information between train and validation\n",
    "\n",
    "augmented_train_data = augmentor.augment(train_data)\n",
    "augmented_val_data = augmentor.augment(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Grow factor: {len(augmented_train_data) / (len(train_data) * SUBSET_PERCENT) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_data.to_json(\"augmented_train.jsonl\", orient=\"records\", lines=True)\n",
    "augmented_val_data.to_json(\"augmented_val.jsonl\", orient=\"records\", lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
